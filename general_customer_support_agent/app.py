# ---------------------------------------------
# Imports & Setup
# ---------------------------------------------
import streamlit as st
import os, time, json, base64, wave, asyncio
import speech_recognition as sr
import boto3
import edge_tts
from pydub import AudioSegment
from pydub.playback import play
from datetime import datetime
import shutil

# Directories
AUDIO_DIR = "audios"
MEMORY_DIR = "memory"
os.makedirs(AUDIO_DIR, exist_ok=True)
os.makedirs(MEMORY_DIR, exist_ok=True)

# AWS + Bedrock (LLaMA 3 70B Instruct)
AWS_REGION = "ap-south-1"
LLAMA_3_MODEL_ID = "meta.llama3-70b-instruct-v1:0"
session = boto3.Session(region_name=AWS_REGION)
bedrock = session.client("bedrock-runtime")

# ---------------------------------------------
# UI Setup
# ---------------------------------------------
st.set_page_config(page_title="Customer Support Agent")
st.title("ЁЯОз Voice Assistant for Customer Support")

lang_map = {
    "English (India)": {
        "voice": "en-IN-NeerjaNeural",
        "stt": "en-IN",
        "greeting": "Hi! Thank you for calling us, How can I assist you today?",
        "goodbye": "Thank you for calling us, we hope to assist you in the future! <END_CALL>"
    },
    "Hindi": {
        "voice": "hi-IN-SwaraNeural",
        "stt": "hi-IN",
        "greeting": "рдирдорд╕реНрддреЗ! рд╣рдореЗрдВ рдХреЙрд▓ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдзрдиреНрдпрд╡рд╛рдж, рдореИрдВ рдЖрдЬ рдЖрдкрдХреА рдХреИрд╕реЗ рд╕рд╣рд╛рдпрддрд╛ рдХрд░ рд╕рдХрддреА рд╣реВрдБ?",
        "goodbye": "рд╣рдореЗрдВ рдХреЙрд▓ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдзрдиреНрдпрд╡рд╛рдж, рд╣рдо рднрд╡рд┐рд╖реНрдп рдореЗрдВ рдЖрдкрдХреА рд╕рд╣рд╛рдпрддрд╛ рдХрд░рдиреЗ рдХреА рдЖрд╢рд╛ рдХрд░рддреЗ рд╣реИрдВ! <END_CALL>"
    },
    "Marathi": {
        "voice": "hi-IN-SwaraNeural",
        "stt": "mr-IN",
        "greeting": "рдирдорд╕реНрдХрд╛рд░, рдЖрдореНрд╣рд╛рд▓рд╛ рдХреЙрд▓ рдХреЗрд▓реНрдпрд╛рдмрджреНрджрд▓ рдзрдиреНрдпрд╡рд╛рдж! рдореА рдЖрдЬ рдЖрдкрд▓реА рдХрд╢реА рдорджрдд рдХрд░реВ?",
        "goodbye": "рдЖрдореНрд╣рд╛рд▓рд╛ рдХреЙрд▓ рдХреЗрд▓реНрдпрд╛рдмрджреНрджрд▓ рдзрдиреНрдпрд╡рд╛рдж, рдЖрдореНрд╣реА рднрд╡рд┐рд╖реНрдпрд╛рддрд╣реА рдЖрдкрд▓реА рдорджрдд рдХрд░рдгреНрдпрд╛рдЪреА рдЖрд╢рд╛ рдХрд░рддреЛ! <END_CALL>"
    },
    "Gujarati": {
        "voice": "hi-IN-SwaraNeural",
        "stt": "gu-IN",
        "greeting": "ркиркорк╕рлНркдрлЗ, ркЕркоркирлЗ ркХрлЛрк▓ ркХрк░рк╡рк╛ ркмркжрк▓ ркЖркнрк╛рк░! ркЖркЬрлЗ рк╣рлБркВ ркХрлЗрк╡рлА рк░рлАркдрлЗ ркдркорк╛рк░рлА ркоркжркж ркХрк░рлА рк╢ркХрлБркВ?",
        "goodbye": "ркЕркоркирлЗ ркХрлЛрк▓ ркХрк░рк╡рк╛ ркмркжрк▓ ркЖркнрк╛рк░, ркнрк╡рк┐рк╖рлНркпркорк╛ркВ рккркг ркдркорк╛рк░рлА рк╕рк╣рк╛ркп ркХрк░рк╡рк╛ркирлА ркЖрк╢рк╛ рк░рк╛ркЦрлАркП ркЫрлАркП! <END_CALL>"
    },
    "Bengali": {
        "voice": "bn-IN-TanishaaNeural",
        "stt": "bn-IN",
        "greeting": "ржиржорж╕рзНржХрж╛рж░, ржЖржорж╛ржжрзЗрж░ ржХрж▓ ржХрж░рж╛рж░ ржЬржирзНржп ржзржирзНржпржмрж╛ржж! ржЖржЬ ржЖржорж┐ ржЖржкржирж╛ржХрзЗ ржХрзАржнрж╛ржмрзЗ рж╕рж╛рж╣рж╛ржпрзНржп ржХрж░рждрзЗ ржкрж╛рж░рж┐?",
        "goodbye": "ржЖржорж╛ржжрзЗрж░ ржХрж▓ ржХрж░рж╛рж░ ржЬржирзНржп ржзржирзНржпржмрж╛ржж, ржнржмрж┐рж╖рзНржпрждрзЗ ржЖржкржирж╛ржХрзЗ рж╕рж╛рж╣рж╛ржпрзНржп ржХрж░рждрзЗ ржкрж╛рж░ржм ржмрж▓рзЗ ржЖрж╢рж╛ ржХрж░рж┐! <END_CALL>"
    },
    "Tamil": {
        "voice": "ta-IN-PallaviNeural",
        "stt": "ta-IN",
        "greeting": "ро╡рогроХрпНроХроорпН, роОроЩрпНроХро│рпИ роЕро┤рпИродрпНродродро▒рпНроХрпБ роиройрпНро▒ро┐! роЗройрпНро▒рпБ роиро╛ройрпН роЙроЩрпНроХро│рпБроХрпНроХрпБ роОрокрпНрокроЯро┐ роЙродро╡ро▓ро╛роорпН?",
        "goodbye": "роОроЩрпНроХро│рпИ роЕро┤рпИродрпНродродро▒рпНроХрпБ роиройрпНро▒ро┐, роОродро┐ро░рпНроХро╛ро▓родрпНродро┐ро▓рпН роЙроЩрпНроХро│рпБроХрпНроХрпБ роЙродро╡ ро╡ро┐ро░рпБроорпНрокрпБроХро┐ро▒рпЛроорпН! <END_CALL>"
    },
    "Telugu": {
        "voice": "te-IN-ShrutiNeural",
        "stt": "te-IN",
        "greeting": "р░ир░ор░╕р▒Нр░др▒З, р░ор░ор▒Нр░ор░▓р▒Нр░ир░┐ р░Хр░╛р░▓р▒Н р░Ър▒Зр░╕р░┐р░ир░Вр░жр▒Бр░Хр▒Б р░зр░ир▒Нр░пр░╡р░╛р░жр░╛р░▓р▒Б! р░ир▒Зр░ир▒Б р░И р░░р▒Лр░Ьр▒Б р░ор▒Ар░Хр▒Б р░Ор░▓р░╛ р░╕р░╣р░╛р░пр░В р░Ър▒Зр░пр░Чр░▓р░ир▒Б?",
        "goodbye": "р░ор░ор▒Нр░ор░▓р▒Нр░ир░┐ р░Хр░╛р░▓р▒Н р░Ър▒Зр░╕р░┐р░ир░Вр░жр▒Бр░Хр▒Б р░зр░ир▒Нр░пр░╡р░╛р░жр░╛р░▓р▒Б, р░нр░╡р░┐р░╖р▒Нр░пр░др▒Нр░др▒Бр░▓р▒Л р░ор▒Зр░ор▒Б р░ор░┐р░ор▒Нр░ор░▓р▒Нр░ир░┐ р░╕р░╣р░╛р░пр░В р░Ър▒Зр░пр░Чр░▓р░ор░ир░┐ р░Жр░╢р░┐р░╕р▒Нр░др▒Бр░ир▒Нр░ир░╛р░ор▒Б! <END_CALL>"
    },
    "Punjabi": {
        "voice": "hi-IN-SwaraNeural",
        "stt": "pa-IN",
        "greeting": "ри╕рид ри╕рйНри░рйА риЕриХри╛ри▓, ри╕ри╛риирйВрй░ риХри╛ри▓ риХри░рии ри▓риИ ризрй░риири╡ри╛риж! риЕрй▒риЬ риорйИриВ ридрйБри╣ри╛рибрйА риХри┐ри╡рйЗриВ риорижриж риХри░ ри╕риХрижри╛ ри╣ри╛риВ?",
        "goodbye": "ри╕ри╛риирйВрй░ риХри╛ри▓ риХри░рии ри▓риИ ризрй░риири╡ри╛риж, риЕри╕рйАриВ ринри╡ри┐рй▒риЦ ри╡ри┐рй▒риЪ ри╡рйА ридрйБри╣ри╛рибрйА риорижриж риХри░рии рижрйА риЙриорйАриж ри░рй▒риЦрижрйЗ ри╣ри╛риВ! <END_CALL>"
    },
    "Odia": {
        "voice": "hi-IN-SwaraNeural",
        "stt": "or-IN",
        "greeting": "рмирморм╕рнНрмХрм╛рм░, рмЖрмормХрнБ рмХрм▓рнН рмХрм░рм┐рммрм╛рмХрнБ рмзрмирнНрнЯрммрм╛рмж! рмЖрмЬрм┐ рморнБрмБ рмЖрмкрмгрмЩрнНрмХрнБ рмХрм┐рмкрм░рм┐ рм╕рм╣рмпрнЛрмЧ рмХрм░рм┐рмкрм╛рм░рм┐рммрм┐?",
        "goodbye": "рмЖрмормХрнБ рмХрм▓рнН рмХрм░рм┐рмерм┐рммрм╛ рмкрм╛рмЗрмБ рмзрмирнНрнЯрммрм╛рмж, рмЖрморнЗ рмЖрмЧрм╛рморнА рмжрм┐рмирм░рнЗ рмЖрмкрмгрмЩрнНрмХрнБ рм╕рм╣рмпрнЛрмЧ рмХрм░рм┐рммрм╛рмХрнБ рмЖрм╢рм╛ рмХрм░рнБрмЫрнБ! <END_CALL>"
    }
}

lang = st.selectbox("Choose Language", list(lang_map.keys()))
lang_data = lang_map[lang]
voice_id = lang_data["voice"]
stt_lang = lang_data["stt"]

# ---------------------------------------------
# Session State
# ---------------------------------------------
if "chat" not in st.session_state: st.session_state.chat = []
if "phase" not in st.session_state: st.session_state.phase = "init"
if "last_msg_hash" not in st.session_state: st.session_state.last_msg_hash = None

# ---------------------------------------------
# LLM (Native Format for LLaMA 3)
# ---------------------------------------------
def get_llm_response(history):
    system_prompt = """You are a polite, multilingual customer support agent. Your job is to 
-assist customer when they call you, provide appropriate solutions to any and all queries, answer generally what you think might help them
-do sentiment analysis and respond accordingly, always be empathetic.
The user will select the language(before starting the call), you have to respond in that language only.
If english language is selected(default) continue in that, do not ask once again to change languages.
Do not change to default messages or languages, continue the same language and be empathetic throughout.
If needed or if they seem unsatisfied, offer to connect them to a human.
Keep the replies short and natural(about 50 words).
Always thank the customer and update the records.
Once the call is concluded and you have thanked the customer, end the call."""

    formatted = "<|begin_of_text|>"
    formatted += f"<|start_header_id|>system<|end_header_id|>\n{system_prompt}<|eot_id|>\n"

    for msg in history:
        role = "user" if msg["role"] == "user" else "assistant"
        formatted += f"<|start_header_id|>{role}<|end_header_id|>\n{msg['content']}<|eot_id|>\n"

    formatted += "<|start_header_id|>assistant<|end_header_id|>\n"

    body = json.dumps({
        "prompt": formatted,
        "max_gen_len": 100,
        "temperature": 0.7
    })

    try:
        response = bedrock.invoke_model(body=body, modelId=LLAMA_3_MODEL_ID)
        output = json.loads(response["body"].read())
        return output["generation"].strip()
    except Exception:
        return "Sorry, something went wrong. <END_CALL>"

def summarize_chat(chat_history):
    formatted = "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nYou are a helpful assistant. Summarize this call in a detailed story format for internal recordkeeping. Be natural and descriptive.<|eot_id|>\n"
    for msg in chat_history:
        role = "user" if msg["role"] == "user" else "assistant"
        formatted += f"<|start_header_id|>{role}<|end_header_id|>\n{msg['content']}<|eot_id|>\n"
    formatted += "<|start_header_id|>assistant<|end_header_id|>\n"

    body = json.dumps({
        "prompt": formatted,
        "max_gen_len": 400,
        "temperature": 0.6
    })

    try:
        response = bedrock.invoke_model(body=body, modelId=LLAMA_3_MODEL_ID)
        output = json.loads(response["body"].read())
        return output["generation"].strip()
    except Exception:
        return "Summary generation failed."

# ---------------------------------------------
# Utilities
# ---------------------------------------------
def hash_message(content):
    return str(abs(hash(content.strip().lower())))

async def synthesize_audio(text, path, voice):
    mp3_path = path.replace(".wav", ".mp3")
    if not os.path.exists(path):
        tts = edge_tts.Communicate(text.split("<")[0], voice=voice, rate="+30%")
        await tts.save(mp3_path)
        AudioSegment.from_file(mp3_path, format="mp3").export(path, format="wav")

def play_audio_blocking(path):
    audio = AudioSegment.from_wav(path)
    play(audio)

def record_user_voice():
    rec = sr.Recognizer()
    with sr.Microphone() as source:
        rec.adjust_for_ambient_noise(source)
        st.toast("ЁЯОз Listening...")
        audio = rec.listen(source, timeout=5, phrase_time_limit=8)
        return rec.recognize_google(audio, language=stt_lang)

def cleanup_audio():
    for file in os.listdir(AUDIO_DIR):
        os.remove(os.path.join(AUDIO_DIR, file))

# ---------------------------------------------
# Status Indicator
# ---------------------------------------------
status = {
    "speak": "ЁЯЧгя╕П Speaking...",
    "listen": "ЁЯОз Listening...",
    "process": "ЁЯТм Thinking...",
    "done": "тЬЕ Call Ended."
}.get(st.session_state.phase, "")
if status: st.info(status)

# ---------------------------------------------
# Start Call
# ---------------------------------------------
if not st.session_state.chat:
    if st.button("Start Call ЁЯОЩя╕П"):
        st.session_state.chat.append({"role": "assistant", "content": lang_data["greeting"]})
        st.session_state.phase = "speak"
        st.rerun()

# ---------------------------------------------
# Display Chat
# ---------------------------------------------
for msg in st.session_state.chat:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"].split("<")[0])

# ---------------------------------------------
# End Call Manually
# ---------------------------------------------
if st.session_state.phase not in ["init", "done"]:
    if st.button("тЭМ End Call"):
        st.session_state.phase = "done"

        summary = summarize_chat(st.session_state.chat)
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        with open(f"{MEMORY_DIR}/summary_{timestamp}.txt", "w", encoding="utf-8") as f:
            f.write(summary)
        with open(f"{MEMORY_DIR}/summary_{timestamp}.json", "w", encoding="utf-8") as f:
            json.dump(st.session_state.chat, f, ensure_ascii=False, indent=2)

        cleanup_audio()
        st.rerun()

# ---------------------------------------------
# Assistant Speaks
# ---------------------------------------------
if st.session_state.phase == "speak":
    last = st.session_state.chat[-1]
    msg_hash = hash_message(last["content"])
    filename = os.path.join(AUDIO_DIR, f"{msg_hash}.wav")

    if st.session_state.last_msg_hash != msg_hash:
        asyncio.run(synthesize_audio(last["content"], filename, voice_id))
        st.session_state.last_msg_hash = msg_hash

    play_audio_blocking(filename)
    st.session_state.phase = "listen"
    st.rerun()

# ---------------------------------------------
# User Speaks
# ---------------------------------------------
if st.session_state.phase == "listen":
    try:
        user_input = record_user_voice()
    except:
        fallback = "Sorry, I couldn't hear you. Could you please repeat that?"
        st.session_state.chat.append({"role": "assistant", "content": fallback})
        st.session_state.phase = "speak"
        st.rerun()

    if user_input and user_input.strip():
        st.session_state.chat.append({"role": "user", "content": user_input})
        st.session_state.phase = "process"
    else:
        fallback = "I didn't catch that. Could you say it again?"
        st.session_state.chat.append({"role": "assistant", "content": fallback})
        st.session_state.phase = "speak"
    st.rerun()

# ---------------------------------------------
# LLM Processes
# ---------------------------------------------
if st.session_state.phase == "process":
    reply = get_llm_response(st.session_state.chat)
    st.session_state.chat.append({"role": "assistant", "content": reply})

    if "<END_CALL>" in reply:
        st.session_state.phase = "done"
        st.success("тЬЕ Call Ended.")

        summary = summarize_chat(st.session_state.chat)
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        with open(f"{MEMORY_DIR}/summary_{timestamp}.txt", "w", encoding="utf-8") as f:
            f.write(summary)
        with open(f"{MEMORY_DIR}/summary_{timestamp}.json", "w", encoding="utf-8") as f:
            json.dump(st.session_state.chat, f, ensure_ascii=False, indent=2)

        cleanup_audio()
    else:
        st.session_state.phase = "speak"
    st.rerun()
