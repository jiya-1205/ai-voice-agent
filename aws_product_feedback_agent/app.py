# ---------------------------------------------
# Imports & Setup
# ---------------------------------------------
import streamlit as st
import os, time, json, base64, wave, asyncio
import speech_recognition as sr
import boto3
import edge_tts
from pydub import AudioSegment
from pydub.playback import play
from datetime import datetime
import shutil

# Directories
AUDIO_DIR = "audios"
MEMORY_DIR = "memory"
os.makedirs(AUDIO_DIR, exist_ok=True)
os.makedirs(MEMORY_DIR, exist_ok=True)

# AWS + Bedrock (LLaMA 3 70B Instruct)
AWS_REGION = "ap-south-1"
LLAMA_3_MODEL_ID = "meta.llama3-70b-instruct-v1:0"
session = boto3.Session(region_name=AWS_REGION)
bedrock = session.client("bedrock-runtime")

# ---------------------------------------------
# UI Setup
# ---------------------------------------------
st.set_page_config(page_title="Eva тАФ Your Voice Assistant")
st.title("ЁЯОз Eva тАФ Your Voice Assistant")

lang_map = {
    "English (India)": {
        "voice": "en-IN-NeerjaNeural",
        "stt": "en-IN",
        "greeting": "Hi! This call is for product feedback. May I know if you recently purchased one of our products?",
        "goodbye": "Thanks for your time. Goodbye! <END_CALL>"
    },
    "Hindi": {
        "voice": "hi-IN-SwaraNeural",
        "stt": "hi-IN",
        "greeting": "рдирдорд╕реНрддреЗ! рдпрд╣ рдХреЙрд▓ рдЙрддреНрдкрд╛рдж рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛ рдХреЗ рд▓рд┐рдП рд╣реИред рдХреНрдпрд╛ рдЖрдкрдиреЗ рд╣рд╛рд▓ рд╣реА рдореЗрдВ рд╣рдорд╛рд░рд╛ рдХреЛрдИ рдЙрддреНрдкрд╛рдж рдЦрд░реАрджрд╛ рд╣реИ?",
        "goodbye": "рдЖрдкрдХреЗ рд╕рдордп рдХреЗ рд▓рд┐рдП рдзрдиреНрдпрд╡рд╛рджред рдЕрд▓рд╡рд┐рджрд╛! <END_CALL>"
    },
    "Marathi": {
        "voice": "hi-IN-SwaraNeural",
        "stt": "mr-IN",
        "greeting": "рдирдорд╕реНрдХрд╛рд░! рд╣рд╛ рдХреЙрд▓ рдЙрддреНрдкрд╛рджрди рдЕрднрд┐рдкреНрд░рд╛рдпрд╛рд╕рд╛рдареА рдЖрд╣реЗ. рддреБрдореНрд╣реА рдЕрд▓реАрдХрдбреЗ рдЖрдордЪрдВ рдХрд╛рд╣реА рдЙрддреНрдкрд╛рджрди рд╡рд┐рдХрдд рдШреЗрддрд▓рдВ рдХрд╛?",
        "goodbye": "рддреБрдордЪреНрдпрд╛ рд╡реЗрд│реЗрд╕рд╛рдареА рдзрдиреНрдпрд╡рд╛рдж. рдЧреБрдбрдмрд╛рдп! <END_CALL>"
    },
    "Gujarati": {
        "voice": "hi-IN-SwaraNeural",
        "stt": "gu-IN",
        "greeting": "ркиркорк╕рлНркдрлЗ! ркЖ ркХрлЛрк▓ рккрлНрк░рлЛркбркХрлНркЯ рккрлНрк░ркдрк┐рк╕рк╛ркж ркорк╛ркЯрлЗ ркЫрлЗ. рк╢рлБркВ ркдркорлЗ ркдрк╛ркЬрлЗркдрк░ркорк╛ркВ ркирлБркВ ркХрлЛркИ рккрлНрк░рлЛркбркХрлНркЯ ркЦрк░рлАркжрлНркпрлБркВ ркЫрлЗ?",
        "goodbye": "ркдркорк╛рк░рк╛ рк╕ркоркп ркорк╛ркЯрлЗ ркЖркнрк╛рк░. ркЕрк▓рк╡рк┐ркжрк╛! <END_CALL>"
    },
    "Bengali": {
        "voice": "bn-IN-TanishaaNeural",
        "stt": "bn-IN",
        "greeting": "ржиржорж╕рзНржХрж╛рж░! ржПржЗ ржХрж▓ржЯрж┐ ржкржгрзНржпрзЗрж░ ржкрзНрж░рждрж┐ржХрзНрж░рж┐ржпрж╝рж╛ ржирзЗржУржпрж╝рж╛рж░ ржЬржирзНржпред ржЖржкржирж┐ ржХрж┐ рж╕ржорзНржкрзНрж░рждрж┐ ржЖржорж╛ржжрзЗрж░ ржХрзЛржирзЛ ржкржгрзНржп ржХрж┐ржирзЗржЫрзЗржи?",
        "goodbye": "ржЖржкржирж╛рж░ рж╕ржоржпрж╝рзЗрж░ ржЬржирзНржп ржзржирзНржпржмрж╛ржжред ржмрж┐ржжрж╛ржпрж╝! <END_CALL>"
    },
    "Tamil": {
        "voice": "ta-IN-PallaviNeural",
        "stt": "ta-IN",
        "greeting": "ро╡рогроХрпНроХроорпН! роЗроирпНрод роЕро┤рпИрокрпНрокрпБ родропро╛ро░ро┐рокрпНрокрпБ рокро┐ройрпНройрпВроЯрпНроЯродрпНродро┐ро▒рпНроХро╛роХ. роирпАроЩрпНроХро│рпН роЪроорпАрокродрпНродро┐ро▓рпН роОроЩрпНроХро│рпН родропро╛ро░ро┐рокрпНрокрпБроХро│рпИ ро╡ро╛роЩрпНроХро┐ропро┐ро░рпБроХрпНроХро┐ро▒рпАро░рпНроХро│ро╛?",
        "goodbye": "роЙроЩрпНроХро│рпН роирпЗро░родрпНродро┐ро▒рпНроХро╛роХ роиройрпНро▒ро┐. роХрпБроЯрпНрокрпИ! <END_CALL>"
    },
    "Telugu": {
        "voice": "te-IN-ShrutiNeural",
        "stt": "te-IN",
        "greeting": "р░ир░ор░╕р▒Нр░др▒З! р░И р░Хр░╛р░▓р▒Н р░Йр░др▒Нр░кр░др▒Нр░др░┐ р░Ер░нр░┐р░кр▒Нр░░р░╛р░пр░В р░Хр▒Лр░╕р░В. р░ор▒Ар░░р▒Б р░Зр░Яр▒Ар░╡р░▓ р░ор░╛ р░Йр░др▒Нр░кр░др▒Нр░др░┐р░ир░┐ р░Хр▒Кр░ир▒Бр░Чр▒Лр░▓р▒Б р░Ър▒Зр░╢р░╛р░░р░╛?",
        "goodbye": "р░ор▒А р░╕р░ор░пр░╛р░ир░┐р░Хр░┐ р░зр░ир▒Нр░пр░╡р░╛р░жр░╛р░▓р▒Б. р░Чр▒Бр░бр▒Н р░мр▒И! <END_CALL>"
    },
    "Punjabi": {
        "voice": "hi-IN-SwaraNeural",
        "stt": "pa-IN",
        "greeting": "ри╕рид ри╕рйНри░рйА риЕриХри╛ри▓! риЗри╣ риХри╛ри▓ риЙридрикри╛риж рикрйНри░ридрйАриХри┐ри░ри┐риЖ ри▓риИ ри╣рйИред риХрйА ридрйБри╕рйАриВ ри╣ри╛ри▓ ри╣рйА ри╡ри┐рй▒риЪ ри╕ри╛рибри╛ риХрйЛриИ риЙридрикри╛риж риЦри░рйАрижри┐риЖ ри╣рйИ?",
        "goodbye": "ридрйБри╣ри╛рибри╛ ри╕риори╛риВ рижрйЗриг ри▓риИ ризрй░риири╡ри╛рижред риЧрйБрй▒рибримри╛риП! <END_CALL>"
    },
    "Odia": {
        "voice": "hi-IN-SwaraNeural",
        "stt": "or-IN",
        "greeting": "рмирморм╕рнНрмХрм╛рм░! рмПрм╣рм┐ рмХрм▓рнНтАМ рмкрнНрм░рнЛрмбрмХрнНрмЯ рмлрм┐рмбрнНтАМрммрнНрнЯрм╛рмХрнН рмкрм╛рмЗрмБред рмЖрмкрмг рм╕рморнНрмкрнНрм░рмдрм┐ рмХрм┐рмЫрм┐ рмЙрмдрнНрмкрм╛рмж рмХрм┐рмгрм┐рмЫрмирнНрмдрм┐ рмХрм┐?",
        "goodbye": "рмЖрмкрмгрмЩрнНрмХ рм╕рморнЯ рмкрм╛рмЗрмБ рмзрмирнНрнЯрммрм╛рмжред рмЧрнБрмбрнНтАМрммрм╛рмП! <END_CALL>"
    }
}

lang = st.selectbox("Choose Language", list(lang_map.keys()))
lang_data = lang_map[lang]
voice_id = lang_data["voice"]
stt_lang = lang_data["stt"]

# ---------------------------------------------
# Session State
# ---------------------------------------------
if "chat" not in st.session_state: st.session_state.chat = []
if "phase" not in st.session_state: st.session_state.phase = "init"
if "last_msg_hash" not in st.session_state: st.session_state.last_msg_hash = None

# ---------------------------------------------
# LLM (Native Format for LLaMA 3)
# ---------------------------------------------
def get_llm_response(history):
    system_prompt = """You are a polite, multilingual customer service agent. 
Your job is to call customers for a product feedback, also to sentiment analysis and respond accordingly.
The user will select the language(before starting the call), you have to respond in that language only.
If english language is selected(default) continue in that, do not ask once again to change languages.
Do not change to default messages or languages, continue the same language and be empathetic throughout.
-Start by greeting them and asking if they recently bought anything,
if not, apologise and conclude the call.
-If they have, check if theyтАЩve used it and ask about their experience,
 if they haven't used it yet, politely urge them to use it, and schedule a call for later.
-If theyтАЩre happy, ask for a review(on the call itself),
If not, ask what could be improved and listen carefully.
If needed, offer to connect them to a human.
Keep the replies short and natural(about 50 words).
Always thank the customer and update the records.
Once the call is concluded and you have thanked the customer, end the call."""

    formatted = "<|begin_of_text|>"
    formatted += f"<|start_header_id|>system<|end_header_id|>\n{system_prompt}<|eot_id|>\n"

    for msg in history:
        role = "user" if msg["role"] == "user" else "assistant"
        formatted += f"<|start_header_id|>{role}<|end_header_id|>\n{msg['content']}<|eot_id|>\n"

    formatted += "<|start_header_id|>assistant<|end_header_id|>\n"

    body = json.dumps({
        "prompt": formatted,
        "max_gen_len": 100,
        "temperature": 0.7
    })

    try:
        response = bedrock.invoke_model(body=body, modelId=LLAMA_3_MODEL_ID)
        output = json.loads(response["body"].read())
        return output["generation"].strip()
    except Exception:
        return "Sorry, something went wrong. <END_CALL>"

def summarize_chat(chat_history):
    formatted = "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nYou are a helpful assistant. Summarize this call in a detailed story format for internal recordkeeping. Be natural and descriptive.<|eot_id|>\n"
    for msg in chat_history:
        role = "user" if msg["role"] == "user" else "assistant"
        formatted += f"<|start_header_id|>{role}<|end_header_id|>\n{msg['content']}<|eot_id|>\n"
    formatted += "<|start_header_id|>assistant<|end_header_id|>\n"

    body = json.dumps({
        "prompt": formatted,
        "max_gen_len": 400,
        "temperature": 0.6
    })

    try:
        response = bedrock.invoke_model(body=body, modelId=LLAMA_3_MODEL_ID)
        output = json.loads(response["body"].read())
        return output["generation"].strip()
    except Exception:
        return "Summary generation failed."

# ---------------------------------------------
# Utilities
# ---------------------------------------------
def hash_message(content):
    return str(abs(hash(content.strip().lower())))

async def synthesize_audio(text, path, voice):
    mp3_path = path.replace(".wav", ".mp3")
    if not os.path.exists(path):
        tts = edge_tts.Communicate(text.split("<")[0], voice=voice, rate="+30%")
        await tts.save(mp3_path)
        AudioSegment.from_file(mp3_path, format="mp3").export(path, format="wav")

def play_audio_blocking(path):
    audio = AudioSegment.from_wav(path)
    play(audio)

def record_user_voice():
    rec = sr.Recognizer()
    with sr.Microphone() as source:
        rec.adjust_for_ambient_noise(source)
        st.toast("ЁЯОз Listening...")
        audio = rec.listen(source, timeout=5, phrase_time_limit=8)
        return rec.recognize_google(audio, language=stt_lang)

def cleanup_audio():
    for file in os.listdir(AUDIO_DIR):
        os.remove(os.path.join(AUDIO_DIR, file))

# ---------------------------------------------
# Status Indicator
# ---------------------------------------------
status = {
    "speak": "ЁЯЧгя╕П Speaking...",
    "listen": "ЁЯОз Listening...",
    "process": "ЁЯТм Thinking...",
    "done": "тЬЕ Call Ended."
}.get(st.session_state.phase, "")
if status: st.info(status)

# ---------------------------------------------
# Start Call
# ---------------------------------------------
if not st.session_state.chat:
    if st.button("Start Call ЁЯОЩя╕П"):
        st.session_state.chat.append({"role": "assistant", "content": lang_data["greeting"]})
        st.session_state.phase = "speak"
        st.rerun()

# ---------------------------------------------
# Display Chat
# ---------------------------------------------
for msg in st.session_state.chat:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"].split("<")[0])

# ---------------------------------------------
# End Call Manually
# ---------------------------------------------
if st.session_state.phase not in ["init", "done"]:
    if st.button("тЭМ End Call"):
        st.session_state.phase = "done"

        summary = summarize_chat(st.session_state.chat)
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        with open(f"{MEMORY_DIR}/summary_{timestamp}.txt", "w", encoding="utf-8") as f:
            f.write(summary)
        with open(f"{MEMORY_DIR}/summary_{timestamp}.json", "w", encoding="utf-8") as f:
            json.dump(st.session_state.chat, f, ensure_ascii=False, indent=2)

        cleanup_audio()
        st.rerun()

# ---------------------------------------------
# Assistant Speaks
# ---------------------------------------------
if st.session_state.phase == "speak":
    last = st.session_state.chat[-1]
    msg_hash = hash_message(last["content"])
    filename = os.path.join(AUDIO_DIR, f"{msg_hash}.wav")

    if st.session_state.last_msg_hash != msg_hash:
        asyncio.run(synthesize_audio(last["content"], filename, voice_id))
        st.session_state.last_msg_hash = msg_hash

    play_audio_blocking(filename)
    st.session_state.phase = "listen"
    st.rerun()

# ---------------------------------------------
# User Speaks
# ---------------------------------------------
if st.session_state.phase == "listen":
    try:
        user_input = record_user_voice()
    except:
        fallback = "Sorry, I couldn't hear you. Could you please repeat that?"
        st.session_state.chat.append({"role": "assistant", "content": fallback})
        st.session_state.phase = "speak"
        st.rerun()

    if user_input and user_input.strip():
        st.session_state.chat.append({"role": "user", "content": user_input})
        st.session_state.phase = "process"
    else:
        fallback = "I didn't catch that. Could you say it again?"
        st.session_state.chat.append({"role": "assistant", "content": fallback})
        st.session_state.phase = "speak"
    st.rerun()

# ---------------------------------------------
# LLM Processes
# ---------------------------------------------
if st.session_state.phase == "process":
    reply = get_llm_response(st.session_state.chat)
    st.session_state.chat.append({"role": "assistant", "content": reply})

    if "<END_CALL>" in reply:
        st.session_state.phase = "done"
        st.success("тЬЕ Call Ended.")

        summary = summarize_chat(st.session_state.chat)
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        with open(f"{MEMORY_DIR}/summary_{timestamp}.txt", "w", encoding="utf-8") as f:
            f.write(summary)
        with open(f"{MEMORY_DIR}/summary_{timestamp}.json", "w", encoding="utf-8") as f:
            json.dump(st.session_state.chat, f, ensure_ascii=False, indent=2)

        cleanup_audio()
    else:
        st.session_state.phase = "speak"
    st.rerun()
